---
layout: post
title: In the future, everyone will be world-famous for 15...people.
date: '2017-01-25T10:04:27-05:00'
tags: []
tumblr_url: http://duncecapsforcats.com/post/156356336047/in-the-future-everyone-will-be-world-famous-for
---
<p><a href="http://joshmillard.com/garkov/" target="_blank">Garkov</a> is a webcomic that uses Markov chains to generate text using old Garfield strips as its input corpus. Inspect element reveals that the text is produced using Markov chains on individual letters.</p><div class="post photo"><img src="http://68.media.tumblr.com/757556905296847fea2c6057b2662153/tumblr_inline_okcbvcqtww1qag9g1_1280.png" alt="garkov"/><img src="http://68.media.tumblr.com/7b86dfe097dcde5a905e09a42f1d132c/tumblr_inline_okcbvyJvBy1qag9g1_1280.png" alt="gorfold"/></div><br/><p><i>What is a Markov chain?</i></p><br/><blockquote><p>A stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.</p></blockquote><br/><p><i>What is a Markov chain?</i></p><div class="post photo"><img src="http://68.media.tumblr.com/fb5b17a23bc91ea62859297d1e6945d0/tumblr_on2qkfaEWZ1vmucwyo1_1280.png" alt="screenshot"/></div><p>To use Markov chains to generate text, you would read the input corpus and for each word, note the word that follows it and at what frequency. You would output text by making each word a random function of its immediate predecessor. The number of previous words is the “order”. Order-1 text is generated by producing the next word as a random function of the current word. Order-2 generation produces the next word as a random function of the previous two-word grouping. And so on. Text generated using higher orders will more closely resemble natural speech.</p><br/><style scoped="scoped">
table {
    border: 0;
    width: 49%;
    float: left;
    }
    tr:nth-child(odd) {background-color:#fff4f4}
    td {padding: 8px;
        text-align: left;
        width: 33%;
      }
</style><table><tr><td>and</td><td>-&gt;</td><td>I</td></tr><tr><td>I</td><td>-&gt;</td><td>ran</td></tr><tr><td>ran</td><td>-&gt;</td><td>I</td></tr><tr><td>I</td><td>-&gt;</td><td>ran</td></tr><tr><td>ran</td><td>-&gt;</td><td>so</td></tr><tr><td>so</td><td>-&gt;</td><td>far</td></tr><tr><td>far</td><td>-&gt;</td><td>away</td></tr><tr><td>away</td><td>-&gt;</td><td>I</td></tr><tr><td>I</td><td>-&gt;</td><td>just</td></tr><tr><td>just</td><td>-&gt;</td><td>ran</td></tr></table><style scoped="scoped">
table {
    border: 0;
    width: 49%;
   float: left;
    }
    tr:nth-child(odd) {background-color:#fff4f4}
    td {padding: 8px;
        text-align: left;
        width: 33%;
      }

</style><table><tr><td>and I</td><td>-&gt;</td><td>ran</td></tr><tr><td>I ran</td><td>-&gt;</td><td>I</td></tr><tr><td>ran I</td><td>-&gt;</td><td>ran</td></tr><tr><td>I ran</td><td>-&gt;</td><td>so</td></tr><tr><td>ran so</td><td>-&gt;</td><td>far</td></tr><tr><td>so far</td><td>-&gt;</td><td>away</td></tr><tr><td>far away</td><td>-&gt;</td><td>I</td></tr><tr><td>away I</td><td>-&gt;</td><td>just</td></tr><tr><td>I just</td><td>-&gt;</td><td>ran</td></tr><tr><td></td><td></td><td></td></tr></table><p style="clear:both;"><br/></p><p>Output will depend on size of your input corpus — the larger the training dataset the better, although too large a sample size and your output will be exactly your input.</p>

<p>Markov chains can be used to generate text to bypass spam filters; model games of chance; certain statistical analysis; and even underlies Google’s PageRank algorithm:</p>

<br/><blockquote><div>The formula uses a model of a random surfer who gets bored after several clicks and switches to a random page. The PageRank value of a page reflects the chance that the random surfer will land on that page by clicking on a link. It  can be understood as a Markov chain in which the states are pages, and the transitions are all equally probable and are the links between pages.</div></blockquote><br/><p>To write my own Markov chain text generator these are the likely steps:</p>

<ul><li>Read input corpus.</li>
<li>Break input corpus into individual words using some kind of regex.</li>
<li>Be able to choose order, say one to four, and build a hash(?) storing the n-order word(s) prefix with its word suffix.</li>
<li>Select and output a prefix followed by its suffix.</li>
<li>In case a prefix has multiple possible suffixes, select one from that set randomly or weighted by statistical frequency.</li>
<li>Create new prefix-suffix pairs until output of predetermined length is completed.</li>
</ul><p>Idea: input corpus is some kind of art theory book, output is your artist statement.</p>
